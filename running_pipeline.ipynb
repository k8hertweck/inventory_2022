<<<<<<< HEAD
<<<<<<< HEAD
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"running_pipeline.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOgIh2YlbAQpTms76pPtlLZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Running Training and Prediction Pipeline\n","---\n","This notebook provides all the commands to reproduce the results of training the models, and prediction on the full corpus.\n","\n","This process does not have to be done to update the inventory, but simply to reproduce the reported results, (this is the process used to produce them in the first place).\n","\n","This pipeline has the following steps:\n","\n","*   Split the manually curated datasets\n","*   Train all models on the classificaiton and NER tasks\n","*   Select the best model for each task\n","*   Evaluate best model for each task on their test sets\n","*   Perform classification of full corpus\n","*   Run NER model on predicted biodata resource papers\n","*   Extract URLs from predicted positives\n","*   Gather extra metadata from predicted positives\n","\n","### ***Warning***:\n","\n","Running the full pipeline trains many models, and their \"checkpoint\" files are quite large (~0.5GB per model, ~15GB in total). Simply running prediction requires much less resources, including storage space.\n","\n","### Other use-cases\n","\n","If you want to compare a new model to the previously compared models, you can add another row to `config/models_info.tsv`. This pipeline will train this model and compare it to the others. If the other trained model checkpoint files are still present from a previous run, they will not be re-trained during the process.\n","\n","# Setup\n","---\n","### Mount Drive\n","\n","First, mount Google Drive to have access to files necessary for the run:\n"],"metadata":{"id":"x4whPVjZZa7x"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"BmwESzXcjXTb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657317692334,"user_tz":420,"elapsed":20265,"user":{"displayName":"Kenneth Schackart","userId":"14619721059788161882"}},"outputId":"c13ed096-fc17-4bf8-a74c-04c18c16059c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/GitHub/inventory_2022\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/GitHub/inventory_2022/"]},{"cell_type":"markdown","source":["Run the make target to install Python dependencies, and download the full corpus that was used during training and evaluation."],"metadata":{"id":"6a7pMnIVbKXE"}},{"cell_type":"code","source":["! make setup\n","# Add command here for getting the full corpus, once we have it archived\n","# For now, manually place a file called \"full_corpus.csv\" into the \"data/\" directory\n","# Manually curated data is already stored in the GitHub repository"],"metadata":{"id":"iBMUW3C0YIz4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Running the pipeline\n","---\n","Now, we are ready to run the pipeline\n","\n","## Previewing what has to be done.\n","\n","The following can be run to get a preview of what has to be done."],"metadata":{"id":"XG8imhT0bms7"}},{"cell_type":"code","source":["! make dryrun"],"metadata":{"id":"L6sCA8z9nQWZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Run it\n","\n","The following cell will run the entire pipeline described above. It takes a while, even with GPU acceleration. Without GPU it will take a very long time, if it is able to finish at all."],"metadata":{"id":"BIyIBNEGcC_u"}},{"cell_type":"code","source":["! make train_and_predict"],"metadata":{"id":"zFSmOvuUnSPE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Results\n","---\n","Once the pipeline has run, there are a few important output files\n","\n","\n","## Final inventory\n","\n","The final inventory, including names, URLS, and metadata is found in the file:\n","*    `data/full_corpus_predictions/urls/predictions.csv`\n","\n","## Model training stats\n","\n","The per-epoch training statistics for all models are in the files:\n","\n","*    `out/ner_train_out/best/*/combined_stats.csv`\n","*    `out/classif_train_out/best/*/combined_stats.csv`\n","\n","## Test set evaluation\n","\n","Performance measures of the best model on the test set are located in the files:\n","\n","*    `out/ner_train_out/best/test_set_evaluation/metrics.csv`\n","*    `out/classif_train_out/best/test_set_evaluation/metrics.csv`\n"],"metadata":{"id":"RsXV-FmxccZN"}}]}
=======
{"cells":[{"cell_type":"markdown","metadata":{"id":"x4whPVjZZa7x"},"source":["# Running Training and Prediction Pipeline\n","---\n","This notebook provides all the commands to reproduce the results of training the models, and prediction on the full corpus.\n","\n","This process does not have to be done to update the inventory, but simply to reproduce the reported results, (this is the process used to produce them in the first place).\n","\n","This pipeline has the following steps:\n","\n","*   Split the manually curated datasets\n","*   Train all models on the classificaiton and NER tasks\n","*   Select the best model for each task\n","*   Evaluate best model for each task on their test sets\n","*   Perform classification of full corpus\n","*   Run NER model on predicted biodata resource papers\n","*   Extract URLs from predicted positives\n","*   Gather extra metadata from predicted positives\n","\n","### ***Warning***:\n","\n","Running the full pipeline trains many models, and their \"checkpoint\" files are quite large (~0.5GB per model, ~15GB in total). Simply running prediction requires much less resources, including storage space.\n","\n","### Other use-cases\n","\n","If you want to compare a new model to the previously compared models, you can add another row to `config/models_info.tsv`. This pipeline will train this model and compare it to the others. If the other trained model checkpoint files are still present from a previous run, they will not be re-trained during the process.\n","\n","# Setup\n","---\n","### Mount Drive\n","\n","First, mount Google Drive to have access to files necessary for the run:\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24593,"status":"ok","timestamp":1657212963312,"user":{"displayName":"Kenneth Schackart","userId":"14619721059788161882"},"user_tz":420},"id":"BmwESzXcjXTb","outputId":"05782260-6468-4222-a0bb-d728e66f3886"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/GitHub/inventory_2022\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/GitHub/inventory_2022/"]},{"cell_type":"markdown","metadata":{"id":"6a7pMnIVbKXE"},"source":["Run the make target to install Python dependencies, and download the full corpus that was used during training and evaluation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iBMUW3C0YIz4"},"outputs":[],"source":["! make setup\n","# Add command here for getting the full corpus, once we have it archived\n","# For now, manually place a file called \"full_corpus.csv\" into the \"data/\" directory\n","# Manually curated data is already stored in the GitHub repository"]},{"cell_type":"markdown","metadata":{"id":"XG8imhT0bms7"},"source":["# Running the pipeline\n","---\n","Now, we are ready to run the pipeline\n","\n","## Previewing what has to be done.\n","\n","The following can be run to get a preview of what has to be done."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L6sCA8z9nQWZ"},"outputs":[],"source":["! make dryrun_reproduction"]},{"cell_type":"markdown","metadata":{"id":"BIyIBNEGcC_u"},"source":["## Run it\n","\n","The following cell will run the entire pipeline described above. It takes a while, even with GPU acceleration. Without GPU it will take a very long time, if it is able to finish at all."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zFSmOvuUnSPE"},"outputs":[],"source":["! make train_and_predict"]},{"cell_type":"markdown","metadata":{"id":"RsXV-FmxccZN"},"source":["# Results\n","---\n","Once the pipeline has run, there are a few important output files\n","\n","\n","## Final inventory\n","\n","The final inventory, including names, URLS, and metadata is found in the file:\n","*    `data/full_corpus_predictions/urls/predictions.csv`\n","\n","## Model training stats\n","\n","The per-epoch training statistics for all models are in the files:\n","\n","*    `out/ner_train_out/best/*/combined_stats.csv`\n","*    `out/classif_train_out/best/*/combined_stats.csv`\n","\n","## Test set evaluation\n","\n","Performance measures of the best model on the test set are located in the files:\n","\n","*    `out/ner_train_out/best/test_set_evaluation/metrics.csv`\n","*    `out/classif_train_out/best/test_set_evaluation/metrics.csv`\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOgIh2YlbAQpTms76pPtlLZ","collapsed_sections":[],"name":"running_pipeline.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
>>>>>>> 100ff151b5077460f090c5954085591ea753820f
=======
{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "name": "running_pipeline.ipynb",
            "provenance": [],
            "collapsed_sections": [],
            "authorship_tag": "ABX9TyOgIh2YlbAQpTms76pPtlLZ"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "gpuClass": "standard"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Running Training and Prediction Pipeline\n",
                "---\n",
                "This notebook provides all the commands to reproduce the results of training the models, and prediction on the full corpus.\n",
                "\n",
                "This process does not have to be done to update the inventory, but simply to reproduce the reported results, (this is the process used to produce them in the first place).\n",
                "\n",
                "This pipeline has the following steps:\n",
                "\n",
                "*   Split the manually curated datasets\n",
                "*   Train all models on the classificaiton and NER tasks\n",
                "*   Select the best model for each task\n",
                "*   Evaluate best model for each task on their test sets\n",
                "*   Perform classification of full corpus\n",
                "*   Run NER model on predicted biodata resource papers\n",
                "*   Extract URLs from predicted positives\n",
                "*   Gather extra metadata from predicted positives\n",
                "\n",
                "### ***Warning***:\n",
                "\n",
                "Running the full pipeline trains many models, and their \"checkpoint\" files are quite large (~0.5GB per model, ~15GB in total). Simply running prediction requires much less resources, including storage space.\n",
                "\n",
                "### Other use-cases\n",
                "\n",
                "If you want to compare a new model to the previously compared models, you can add another row to `config/models_info.tsv`. This pipeline will train this model and compare it to the others. If the other trained model checkpoint files are still present from a previous run, they will not be re-trained during the process.\n",
                "\n",
                "# Setup\n",
                "---\n",
                "### Mount Drive\n",
                "\n",
                "First, mount Google Drive to have access to files necessary for the run:\n"
            ],
            "metadata": {
                "id": "x4whPVjZZa7x"
            }
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "id": "BmwESzXcjXTb",
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "executionInfo": {
                    "status": "ok",
                    "timestamp": 1657212963312,
                    "user_tz": 420,
                    "elapsed": 24593,
                    "user": {
                        "displayName": "Kenneth Schackart",
                        "userId": "14619721059788161882"
                    }
                },
                "outputId": "05782260-6468-4222-a0bb-d728e66f3886"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Mounted at /content/drive\n",
                        "/content/drive/MyDrive/GitHub/inventory_2022\n"
                    ]
                }
            ],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "%cd /content/drive/MyDrive/GitHub/inventory_2022/"
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "Run the make target to install Python dependencies, and download the full corpus that was used during training and evaluation."
            ],
            "metadata": {
                "id": "6a7pMnIVbKXE"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "! make setup\n",
                "# Add command here for getting the full corpus, once we have it archived\n",
                "# For now, manually place a file called \"full_corpus.csv\" into the \"data/\" directory\n",
                "# Manually curated data is already stored in the GitHub repository"
            ],
            "metadata": {
                "id": "iBMUW3C0YIz4"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Running the pipeline\n",
                "---\n",
                "Now, we are ready to run the pipeline\n",
                "\n",
                "## Previewing what has to be done.\n",
                "\n",
                "The following can be run to get a preview of what has to be done."
            ],
            "metadata": {
                "id": "XG8imhT0bms7"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "! make dryrun"
            ],
            "metadata": {
                "id": "L6sCA8z9nQWZ"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Run it\n",
                "\n",
                "The following cell will run the entire pipeline described above. It takes a while, even with GPU acceleration. Without GPU it will take a very long time, if it is able to finish at all."
            ],
            "metadata": {
                "id": "BIyIBNEGcC_u"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "! make train_and_predict"
            ],
            "metadata": {
                "id": "zFSmOvuUnSPE"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Results\n",
                "---\n",
                "Once the pipeline has run, there are a few important output files\n",
                "\n",
                "\n",
                "## Final inventory\n",
                "\n",
                "The final inventory, including names, URLS, and metadata is found in the file:\n",
                "*    `data/full_corpus_predictions/urls/predictions.csv`\n",
                "\n",
                "## Model training stats\n",
                "\n",
                "The per-epoch training statistics for all models are in the files:\n",
                "\n",
                "*    `out/ner_train_out/best/*/combined_stats.csv`\n",
                "*    `out/classif_train_out/best/*/combined_stats.csv`\n",
                "\n",
                "## Test set evaluation\n",
                "\n",
                "Performance measures of the best model on the test set are located in the files:\n",
                "\n",
                "*    `out/ner_train_out/best/test_set_evaluation/metrics.csv`\n",
                "*    `out/classif_train_out/best/test_set_evaluation/metrics.csv`\n"
            ],
            "metadata": {
                "id": "RsXV-FmxccZN"
            }
        }
    ]
}
>>>>>>> 813f753139b91c20ff027d7d8e10a2854d28b89b
